{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Different ChatGPTs, to see whether the SOTA chatbots perform well on 0-shot and few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# read API key from local machine\n",
    "with open(\"/home/andrej/Documents/open_ai/made-with-ml-key.txt\", \"r\") as file:\n",
    "    api_key = file.read()\n",
    "    api_key = re.sub(r'\\s+', '', api_key)\n",
    "\n",
    "import openai\n",
    "openai.api_key = api_key\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment training dataset from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pandas also relies on numpy for random sampling\n",
    "np.random.seed(42)  # the answer to everything\n",
    "\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_labeled(df):\n",
    "    single_labels = df['labels'].apply(lambda x: x if len(x) <= 1 else None)\n",
    "    single_labels = single_labels.dropna()  #  leave out the multilabeled ones\n",
    "\n",
    "    # extract the singlelabeled data by index via iloc\n",
    "    single_df = df.iloc[single_labels.index]  \n",
    "\n",
    "    # transform the singlelabeled data labels from list (e.g. [8]) into int (e.g. 8)\n",
    "    single_df['labels'] = single_df['labels'].apply(lambda x: x[0])\n",
    "\n",
    "    return single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['train'].features['labels'].feature.names\n",
    "\n",
    "# labels are by default in a list, filter them and reassign them to an integer instead\n",
    "long_train_df = pd.DataFrame(dataset['train'])\n",
    "train_df = get_single_labeled(long_train_df)\n",
    "\n",
    "long_test_df = pd.DataFrame(dataset['test'])\n",
    "test_df = get_single_labeled(long_test_df)\n",
    "before_adding = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()\n",
    "test_df.info()\n",
    "\n",
    "# adding a few more made up grief comments to the testing dataset, there are only 2 examples labeled grief...\n",
    "grief_additionals = [\n",
    "    \"I still can\\'t bring myself to sit in his chair. Every time I walk past it, I just feel this overwhelming emptiness.\",\n",
    "    \"Every time I walk through the house, I get hit with memories that make me cry all over again. It\\'s like a never-ending cycle.\",\n",
    "    \"I keep reaching for my phone to text him, then remember he\\'s gone. It\\'s like a punch in the gut every single time.\",\n",
    "    \"The holidays used to be my favorite time of year, but now they\\'re just painful reminders of the family we\\'ve lost.\",\n",
    "    \"Sometimes I find myself holding onto his old jacket because it still smells like him. It\\'s comforting and heartbreaking all at once.\"\n",
    "    ]\n",
    "\n",
    "new_entries = {\"text\": [],\n",
    "               \"labels\": [],\n",
    "               \"id\": []\n",
    "               }\n",
    "\n",
    "for id, text in enumerate(grief_additionals):\n",
    "    new_entries[\"text\"].append(text)\n",
    "    new_entries[\"labels\"].append(16)  # the id for \"grief\" label\n",
    "    new_entries[\"id\"].append(f\"manual_id_{id}\")\n",
    "\n",
    "if before_adding:\n",
    "    new_entries_df = pd.DataFrame(new_entries)\n",
    "    test_df = pd.concat([test_df, new_entries_df], ignore_index=True)\n",
    "    before_adding = False\n",
    "    \n",
    "test_df.query('labels == 16').shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dict(df, n_samples_per_category=1):\n",
    "    test_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        # print(f\"Sampling label: {label} with label id: {i}\")\n",
    "        # print(f\"There is \", df.query(f'labels == {i}').shape[0], f\" of {label} in the Test set.\")\n",
    "        sample = df.query(f'labels == {i}').sample(n_samples_per_category)\n",
    "        test_dict[label] = list(sample['text'])\n",
    "    return test_dict\n",
    "\n",
    "\n",
    "def sample_few_shot(df):\n",
    "    \"\"\"Samples one random item for each label from a given DataFrame.\"\"\"\n",
    "    few_shot_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        sample = df.query(f'labels == {i}').sample(1)\n",
    "        while \"[NAME]\" in sample.text.item():\n",
    "            sample = df.query(f'labels == {i}').sample(1)\n",
    "\n",
    "        few_shot_dict[label] = sample.text.item()\n",
    "    return few_shot_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_train = sample_few_shot(train_df)\n",
    "test_dict = create_test_dict(test_df, 7)\n",
    "print(f\"Few-shot training data:\\n{few_shot_train}\")\n",
    "print(f\"Sampled benchmark dataset:\\n{test_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the OpenAI API GPT setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tag(model, system_content=\"\", assistant_content=\"\", user_content=\"\"):\n",
    "    try:\n",
    "        # Get response from OpenAI\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        predicted_tag = response.choices[0].message.content\n",
    "        return predicted_tag\n",
    "\n",
    "    except (openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot testing of various GPT models by OpenAI for benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 3.5-turbo\n",
    "> NOTE: This model will soon be deprecated by OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = \"gpt-3.5-turbo\"\n",
    "system_content = f\"\"\"\n",
    "You are an NLP sentiment prediction tool. Your goal is to predict a label given an input sequence by the user.\n",
    "You must choose between one of the following labels for each input: {labels}.\n",
    "Only respond with the label name and nothing else.\"\"\"\n",
    "print(f\"The system content is:\\n{system_content}\")\n",
    "\n",
    "assistant_content = \"\"\n",
    "user_content = test_dict['admiration'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_gpt(model, labels, data_dict, few_shot_str=None):\n",
    "    gpt_eval = {\"original_labels\": [],\n",
    "                  \"text\": [],\n",
    "                  \"prediction_labels\":[],\n",
    "                  \"model\": model}\n",
    "    \n",
    "    system_content = f\"\"\"\n",
    "        You are an NLP sentiment prediction tool. Your goal is to predict a label given an input sequence by the user.\n",
    "        You must choose between one of the following labels for each input: {labels}.\n",
    "        Only respond with the label name and nothing else.\"\"\"\n",
    "    \n",
    "    if few_shot_str is not None:\n",
    "        system_content = few_shot_str\n",
    "    for label in tqdm(labels):\n",
    "        if label == \"other\":\n",
    "            continue\n",
    "        for instance in data_dict[label]:\n",
    "            user_content = instance\n",
    "            pred = get_tag(model,\n",
    "                    system_content=system_content,\n",
    "                    user_content=user_content)\n",
    "            gpt_eval[\"original_labels\"].append(label)\n",
    "            gpt_eval[\"text\"].append(instance)\n",
    "            gpt_eval[\"prediction_labels\"].append(pred)\n",
    "    \n",
    "    return gpt_eval\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the chosen subset of the testing dataset with GPT4o, GPT4, and GPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# model = \"gpt-3.5-turbo-0125\"\n",
    "# model = \"gpt-4o-2024-05-13\"\n",
    "model = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "# NOTE: Make sure that you are able to save to the given path (anaconda tbrougb jupyter vscod is problematic)\n",
    "benchmarks_path = \"/home/andrej/Code/story-vibe/benchmarks/GPT_evals\"\n",
    "\n",
    "dummy_eval = {\"original_labels\": [],\n",
    "                  \"text\": [],\n",
    "                  \"prediction_labels\":[],\n",
    "                  \"model\": model}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_predictions(y_pred, tags, default=\"other\"):\n",
    "    for i, item in enumerate(y_pred):\n",
    "        if item not in tags:  # hallucinations\n",
    "            y_pred[i] = default\n",
    "        if item.startswith(\"'\") and item.endswith(\"'\"):  # GPT 4 likes to places quotes\n",
    "            y_pred[i] = item[1:-1]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns; sns.set_theme()\n",
    "\n",
    "\n",
    "def plot_tag_dist(y_true, y_pred, name=\"Model\"):\n",
    "    # Distribution of tags\n",
    "    true_tag_freq = dict(Counter(y_true))\n",
    "    pred_tag_freq = dict(Counter(y_pred))\n",
    "    \n",
    "    df_true = pd.DataFrame({\"Label\": list(true_tag_freq.keys()), \"Freq\": list(true_tag_freq.values()), \"source\": \"true\"})\n",
    "    df_pred = pd.DataFrame({\"Label\": list(pred_tag_freq.keys()), \"Freq\": list(pred_tag_freq.values()), \"source\": \"pred\"})\n",
    "    df = pd.concat([df_true, df_pred], ignore_index=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title(name, fontsize=14)\n",
    "    ax = sns.barplot(x=\"Label\", y=\"Freq\", hue=\"source\", data=df)\n",
    "    ax.set_xticklabels(list(true_tag_freq.keys()), rotation=45, fontsize=8, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, name=\"Model\"):\n",
    "    print(f\"len(y_true): {len(y_true)}\")\n",
    "    print(f\"len(y_pred): {len(y_pred)}\")\n",
    "    cfm = np.zeros((len(labels), len(labels)))\n",
    "    if \"other\" not in labels:\n",
    "        labels.append(\"other\")\n",
    "    for id, y_t in enumerate(y_true):\n",
    "\n",
    "        y_p = y_pred[id]  # gets synchronized word label from the prediciton list\n",
    "        y_t_label_id = labels.index(y_t)\n",
    "        y_p_label_id = labels.index(y_p)\n",
    "        cfm[y_t_label_id, y_p_label_id] += 1\n",
    "\n",
    "    conf_matrix_df = pd.DataFrame(cfm, index=labels, columns=labels)\n",
    "\n",
    "    # Plot the confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix_df, cmap='Blues')\n",
    "    plt.title(f'Sentiment Confusion Matrix for {name}')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cfm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "with open(f\"{benchmarks_path}/zero_shot/gpt-3.5-turbo-0125.json\") as gpt35_file:\n",
    "    gpt35_eval = json.load(gpt35_file)\n",
    "\n",
    "with open(f\"{benchmarks_path}/zero_shot/gpt-4-turbo-2024-04-09.json\") as gpt4t_file:\n",
    "    gpt4t_eval = json.load(gpt4t_file)\n",
    "\n",
    "with open(f\"{benchmarks_path}/zero_shot/gpt-4o-2024-05-13.json\") as gpt4o_file:\n",
    "    gpt4o_eval = json.load(gpt4o_file)\n",
    "\n",
    "def plot_benchmark(eval_dict, labels, name=\"Model\"):\n",
    "\n",
    "    # load benchmarks\n",
    "    y_pred = eval_dict['prediction_labels']\n",
    "    y_pred_cleaned = clean_predictions(y_pred, labels)\n",
    "\n",
    "    y_true = eval_dict['original_labels']\n",
    "    if \"other\" not in y_true:\n",
    "        y_true.append(\"other\")\n",
    "        \n",
    "    plot_confusion_matrix(y_true, y_pred, labels=labels, name=name)\n",
    "\n",
    "    plot_tag_dist(y_true, y_pred_cleaned, name=name)\n",
    "\n",
    "plot_benchmark(gpt35_eval, labels, name=\"GPT 3.5\")\n",
    "plot_benchmark(gpt4t_eval, labels, name=\"GPT 4-turbo\")\n",
    "plot_benchmark(gpt4o_eval, labels, name=\"GPT 4o\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot benchmark of GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_string = f\"\"\"You are an NLP sentiment prediction tool. Your goal is to predict a label given an input sequence by the user.\n",
    "You must choose between one of the following labels for each input: {labels}.\n",
    "Only respond with the label name and nothing else.\n",
    "Now follow some few-shot training examples. There will be one example for each label given before:\\n\\n\"\"\"\n",
    "\n",
    "for fs_key, fs_data in few_shot_train.items():\n",
    "    # print(f\"{fs_key}: {fs_data}\")\n",
    "    few_shot_string += f\"{fs_key}: {fs_data}\\n\"\n",
    "print(few_shot_string)\n",
    "\n",
    "# model = \"gpt-3.5-turbo-0125\"\n",
    "# model = \"gpt-4o-2024-05-13\"\n",
    "model = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "models = [\"gpt-3.5-turbo-0125\",\n",
    "          \"gpt-4o-2024-05-13\",\n",
    "          \"gpt-4-turbo-2024-04-09\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{benchmarks_path}/few_shot/gpt-3.5-turbo-0125.json\") as gpt35_file:\n",
    "    fs_gpt35_eval = json.load(gpt35_file)\n",
    "\n",
    "with open(f\"{benchmarks_path}/few_shot/gpt-4-turbo-2024-04-09.json\") as gpt4t_file:\n",
    "    fs_gpt4t_eval = json.load(gpt4t_file)\n",
    "\n",
    "with open(f\"{benchmarks_path}/few_shot/gpt-4o-2024-05-13.json\") as gpt4o_file:\n",
    "    fs_gpt4o_eval = json.load(gpt4o_file)\n",
    "\n",
    "def plot_benchmark(eval_dict, labels, name=\"Model\"):\n",
    "\n",
    "    # load benchmarks\n",
    "    y_pred = eval_dict['prediction_labels']\n",
    "    y_pred_cleaned = clean_predictions(y_pred, labels)\n",
    "\n",
    "    y_true = eval_dict['original_labels']\n",
    "        \n",
    "    plot_confusion_matrix(y_true, y_pred, labels=labels, name=name)\n",
    "    \n",
    "    if \"other\" not in y_true:\n",
    "        y_true.append(\"other\")\n",
    "\n",
    "    plot_tag_dist(y_true, y_pred_cleaned, name=name)\n",
    "\n",
    "plot_benchmark(fs_gpt35_eval, labels, name=\"Few Shot GPT 3.5\")\n",
    "plot_benchmark(fs_gpt4t_eval, labels, name=\"Few Shot GPT 4-turbo\")\n",
    "plot_benchmark(fs_gpt4o_eval, labels, name=\"Few Shot GPT 4o\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "story-vibe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
