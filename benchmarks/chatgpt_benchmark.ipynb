{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Different ChatGPTs, to see whether the SOTA chatbots perform well on 0-shot and few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# read API key from local machine\n",
    "with open(\"/home/andrej/Documents/open_ai/made-with-ml-key.txt\", \"r\") as file:\n",
    "    api_key = file.read()\n",
    "    api_key = re.sub(r'\\s+', '', api_key)\n",
    "\n",
    "import openai\n",
    "openai.api_key = api_key\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment training dataset from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pandas also relies on numpy for random sampling\n",
    "np.random.seed(42)  # the answer to everything\n",
    "\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_labeled(df):\n",
    "    single_labels = df['labels'].apply(lambda x: x if len(x) <= 1 else None)\n",
    "    single_labels = single_labels.dropna()  #  leave out the multilabeled ones\n",
    "\n",
    "    # extract the singlelabeled data by index via iloc\n",
    "    single_df = df.iloc[single_labels.index]  \n",
    "\n",
    "    # transform the singlelabeled data labels from list (e.g. [8]) into int (e.g. 8)\n",
    "    single_df['labels'] = single_df['labels'].apply(lambda x: x[0])\n",
    "\n",
    "    return single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['train'].features['labels'].feature.names\n",
    "\n",
    "# labels are by default in a list, filter them and reassign them to an integer instead\n",
    "long_train_df = pd.DataFrame(dataset['train'])\n",
    "train_df = get_single_labeled(long_train_df)\n",
    "\n",
    "long_test_df = pd.DataFrame(dataset['test'])\n",
    "test_df = get_single_labeled(long_test_df)\n",
    "before_adding = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()\n",
    "test_df.info()\n",
    "\n",
    "# adding a few more made up grief comments to the testing dataset, there are only 2 examples labeled grief...\n",
    "grief_additionals = [\n",
    "    \"I still can\\'t bring myself to sit in his chair. Every time I walk past it, I just feel this overwhelming emptiness.\",\n",
    "    \"Every time I walk through the house, I get hit with memories that make me cry all over again. It\\'s like a never-ending cycle.\",\n",
    "    \"I keep reaching for my phone to text him, then remember he\\'s gone. It\\'s like a punch in the gut every single time.\",\n",
    "    \"The holidays used to be my favorite time of year, but now they\\'re just painful reminders of the family we\\'ve lost.\",\n",
    "    \"Sometimes I find myself holding onto his old jacket because it still smells like him. It\\'s comforting and heartbreaking all at once.\"\n",
    "    ]\n",
    "\n",
    "new_entries = {\"text\": [],\n",
    "               \"labels\": [],\n",
    "               \"id\": []\n",
    "               }\n",
    "\n",
    "for id, text in enumerate(grief_additionals):\n",
    "    new_entries[\"text\"].append(text)\n",
    "    new_entries[\"labels\"].append(16)  # the id for \"grief\" label\n",
    "    new_entries[\"id\"].append(f\"manual_id_{id}\")\n",
    "\n",
    "if before_adding:\n",
    "    new_entries_df = pd.DataFrame(new_entries)\n",
    "    test_df = pd.concat([test_df, new_entries_df], ignore_index=True)\n",
    "    before_adding = False\n",
    "    \n",
    "test_df.query('labels == 16').shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dict(df, n_samples_per_category=1):\n",
    "    test_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        # print(f\"Sampling label: {label} with label id: {i}\")\n",
    "        # print(f\"There is \", df.query(f'labels == {i}').shape[0], f\" of {label} in the Test set.\")\n",
    "        sample = df.query(f'labels == {i}').sample(n_samples_per_category)\n",
    "        test_dict[label] = list(sample['text'])\n",
    "    return test_dict\n",
    "\n",
    "\n",
    "def sample_few_shot(df):\n",
    "    \"\"\"Samples one random item for each label from a given DataFrame.\"\"\"\n",
    "    few_shot_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        sample = df.query(f'labels == {i}').sample(1)\n",
    "        while \"[NAME]\" in sample.text.item():\n",
    "            sample = df.query(f'labels == {i}').sample(1)\n",
    "\n",
    "        few_shot_dict[label] = sample.text.item()\n",
    "    return few_shot_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_train = sample_few_shot(train_df)\n",
    "test_dict = create_test_dict(test_df, 7)\n",
    "print(f\"Few-shot training data:\\n{few_shot_train}\")\n",
    "print(f\"Sampled benchmark dataset:\\n{test_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the OpenAI API GPT setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tag(model, system_content=\"\", assistant_content=\"\", user_content=\"\"):\n",
    "    try:\n",
    "        # Get response from OpenAI\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        predicted_tag = response.choices[0].message.content\n",
    "        return predicted_tag\n",
    "\n",
    "    except (openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot testing of various GPT models by OpenAI for benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 3.5-turbo\n",
    "> NOTE: This model will soon be deprecated by OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = \"gpt-3.5-turbo\"\n",
    "system_content = f\"\"\"\n",
    "You are an NLP sentiment prediction tool. Your goal is to predict a label given an input sequence by the user.\n",
    "You must choose between one of the following labels for each input: {labels}.\n",
    "Only respond with the label name and nothing else.\"\"\"\n",
    "print(f\"The system content is:\\n{system_content}\")\n",
    "\n",
    "assistant_content = \"\"\n",
    "user_content = test_dict['admiration'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt(model, labels, data_dict, few_shot_str=None):\n",
    "    gpt_eval = {\"original_labels\": [],\n",
    "                  \"text\": [],\n",
    "                  \"prediction_labels\":[],\n",
    "                  \"model\": model}\n",
    "    \n",
    "    system_content = f\"\"\"\n",
    "        You are an NLP sentiment prediction tool. Your goal is to predict a label given an input sequence by the user.\n",
    "        You must choose between one of the following labels for each input: {labels}.\n",
    "        Only respond with the label name and nothing else.\"\"\"\n",
    "    assistant_content = \"\"\n",
    "    for label in labels:\n",
    "        for instance in test_dict[label]:\n",
    "            user_content = instance\n",
    "            pred = get_tag(model,\n",
    "                    system_content=system_content,\n",
    "                    user_content=user_content)\n",
    "            gpt_eval[\"original_labels\"].append(label)\n",
    "            gpt_eval[\"text\"].append(instance)\n",
    "            gpt_eval[\"prediction_labels\"].append(pred)\n",
    "    \n",
    "    return gpt_eval\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the chosen subset of the testing dataset with GPT4o, GPT4, and GPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# model = \"gpt-3.5-turbo-0125\"\n",
    "# model = \"gpt-4o-2024-05-13\"\n",
    "model = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "# NOTE: Make sure that you are able to save to the given path (anaconda tbrougb jupyter vscod is problematic)\n",
    "# benchmarks_path = \"benchmarks/GPT_evals\"\n",
    "benchmarks_path = \"/home/andrej/Code/story-vibe/benchmarks/GPT_evals\"\n",
    "\n",
    "dummy_eval = {\"original_labels\": [],\n",
    "                  \"text\": [],\n",
    "                  \"prediction_labels\":[],\n",
    "                  \"model\": model}\n",
    "\n",
    "# zero shot testing first\n",
    "gpt_eval = test_gpt(model=model, labels=labels, data_dict=test_dict)\n",
    "print(f\"gpt_eval:\\n{gpt_eval}\")\n",
    "with open(f\"{benchmarks_path}/zero_shot/{model}.json\", 'w') as gpt_json_file:\n",
    "    json.dump(gpt_eval, gpt_json_file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "story-vibe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
