{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ & üì•: Loading Tokenizer, downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "labels = dataset['train'].features['labels'].feature.names\n",
    "\n",
    "MODEL = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä -> üóÉÔ∏è: Dataset preparation\n",
    "> Rearrange 28 GoEmotions sentiment classes into 6 (+ neutral = 7) basic human emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_labeled(df):\n",
    "    single_labels = df['labels'].apply(lambda x: x if len(x) == 1 else None)\n",
    "    single_labels = single_labels.dropna()  # Leave out the multilabeled ones\n",
    "\n",
    "    # Extract the single-labeled data by index via iloc and create a copy to avoid the SettingWithCopyWarning\n",
    "    single_df = df.iloc[single_labels.index].copy()\n",
    "\n",
    "    # Transform the single-labeled data labels from list (e.g. [8]) into int (e.g. 8)\n",
    "    single_df['labels'] = single_df['labels'].apply(lambda x: x[0])\n",
    "\n",
    "    return single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole dataset into Pandas DataFrame\n",
    "# use only data with single labels, transform labels from list to int\n",
    "train_df = get_single_labeled(pd.DataFrame(dataset['train']))\n",
    "val_df = get_single_labeled(pd.DataFrame(dataset['validation']))\n",
    "test_df = get_single_labeled(pd.DataFrame(dataset['test']))\n",
    "\n",
    "original_labels = dataset['train'].features['labels'].feature.names\n",
    "# 6 basic emotion types | Sadness, Happiness, Fear, Anger, Surprise and Disgust\n",
    "new_labels = ['sadness', 'happiness', 'fear', 'anger', 'surprise', 'disgust', \"neutral\"]\n",
    "\n",
    "# regroup the emotions into 6 basic emotion types\n",
    "labels_reordering = {\n",
    "    'sadness':   ['grief', 'disappointment', 'remorse', 'sadness'],\n",
    "    'happiness': ['admiration', 'amusement', 'approval', 'caring', 'excitement', 'gratitude', 'joy', 'love', 'optimism', 'pride', 'relief', 'desire'],\n",
    "    'fear':      ['fear', 'nervousness'],\n",
    "    'anger':     ['anger', 'annoyance', 'disapproval'],\n",
    "    'surprise':  ['surprise', 'realization', 'confusion', 'curiosity'],\n",
    "    'disgust':   ['disgust', 'embarrassment'],\n",
    "    \"neutral\":   [\"neutral\"]\n",
    "}\n",
    "\n",
    "# regroup into smaller subset of string labels\n",
    "for key, vals in labels_reordering.items():\n",
    "    # key_id = new_labels.index(key)\n",
    "    val_ids = [original_labels.index(val) for val in vals]\n",
    "    for val_id in val_ids:\n",
    "        # first replace for string keys to avoid mixing the new and old labels\n",
    "        train_df.loc[train_df['labels'] == val_id, 'labels'] = key\n",
    "        val_df.loc[val_df['labels'] == val_id, 'labels'] = key\n",
    "        test_df.loc[test_df['labels'] == val_id, 'labels'] = key\n",
    "\n",
    "# change string labels for indices\n",
    "for key in new_labels:\n",
    "    key_id = new_labels.index(key)\n",
    "    train_df.loc[train_df['labels'] == key, 'labels'] = key_id\n",
    "    val_df.loc[val_df['labels'] == key, 'labels'] = key_id\n",
    "    test_df.loc[test_df['labels'] == key, 'labels'] = key_id\n",
    "\n",
    "id2label = {id:label for id, label in enumerate(new_labels)}\n",
    "label2id = {label:id for id, label in enumerate(new_labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è & üìö: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_PROCS = 8\n",
    "LR = 0.00005\n",
    "EPOCHS = 5\n",
    "OUT_DIR = 'story_vibe_output'\n",
    "\n",
    "# turn the preprocessed dataset back to Dataset format to use the tokenization function as is\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, num_proc=NUM_PROCS)\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, num_proc=NUM_PROCS)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True, batch_size=BATCH_SIZE, num_proc=NUM_PROCS)\n",
    "\n",
    "\"\"\"\n",
    "As real-world sentences vary in length, we pad shorter sentences with a special padding token.\n",
    "DataCollatorWithPadding ensures this happens automatically during training.\n",
    "By feeding it, our tokenizer, knows the appropriate padding token and max length to use.\n",
    "\"\"\"\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    report_to='tensorboard',\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "story-vibe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
